{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear regression analysis can help a builder to predict how much houses it would sell in the coming months and at what price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/house_price_pred.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An organisation can use linear regression to figure out how much they would pay to a new employee based on the years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/salary_pred.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a $n\\times p$ and $n\\times1$ matrices. Let's denote them X and Y respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ may be seen as a matrix of row-vectors ${\\displaystyle \\mathbf {x} _{i}}$ or of n-dimensional column-vectors ${\\displaystyle X_{j}}$, which are known as features, regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables  \n",
    "\n",
    "![title](../Images/X.svg?sanitize=true)\n",
    "\n",
    "$\\mathbf {y}$  is a vector of observed values ${\\displaystyle y_{i}\\ (i=1,\\ldots ,n)}$ of the variable called the target, regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable\n",
    "![title](../Images/Y.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######                                     So we need to find some $f:X->Y$, which can do \"good\" predictions on our dataset \n",
    "<br>\n",
    "This $f:X->Y$ function called model.\n",
    "<br>\n",
    "Let's consider that $f$ is a linear function which depends on X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>$f(x_{i}):=\\hat{y}_i=x_{i}\\beta$<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "<br>\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    " \\beta_{0} & \\beta_{1} & \\cdots & \\beta_{p}\n",
    " \\end{bmatrix}^{T}\n",
    "$$\n",
    "\n",
    "Assumption 0.\n",
    "<br>\n",
    "Linearity. This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$L(Y,X,\\beta) = \\frac {1}{2n}\\sum_{i=1}^n(\\hat{y}_i - y_i)^2$$\n",
    "![title](../Images/mse_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a square of Euclidian distance between $Y$(target) and $\\hat{Y}$(predictions) vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See connection with Maximum Likelihood Estimation in Bishop [book](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Our goal is to minimize $L(Y,X,\\beta)$ with respect to $\\beta$ <center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/optimal_beta.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution \n",
    "![title](../Images/Loss_norm.svg?sanitize=true)\n",
    "<br>\n",
    "![title](../Images/loss_der.svg?sanitize=true)\n",
    "<br>\n",
    "![title](../Images/optimal_beta_sol.svg?sanitize=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Whether $X^{T}X$ is always invertible?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption 1. $X$ must have full column rank p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a first-order iterative optimization algorithm for finding the local minimum of a differentiable function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is based on the observation that if the multi-variable function $F(\\mathbf {x} )$ is defined and differentiable in a neighborhood of a point $\\mathbf {a}$ , then $F(\\mathbf {x})$ decreases fastest if one goes from $\\mathbf {a}$  in the direction of the negative gradient of $F$ at  $a$ ,$-\\nabla F(\\mathbf {a})$ It follows that, if\n",
    "<br>\n",
    "<center> $\\mathbf {a} _{n+1}=\\mathbf {a} _{n}-\\gamma \\nabla F(\\mathbf {a} _{n})$ <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/gradient_descent_homo.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/gradient_desc.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../Images/lrning_rate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $\\nabla{L}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla{L} = \\frac{\\partial L}{\\partial \\beta} = \\frac{\\partial \\frac {1}{2n}\\sum_{i=1}^n(\\hat{y}_i - y_i)^2} \n",
    "{\\partial \\beta}\n",
    "= \\frac{1}{n}\\sum_{i=1}^n(x_{i}\\beta - y_i)\\frac{\\partial (x_{i}\\beta - y_i)}{\\partial \\beta} n\n",
    "= \\frac{1}{n}\\sum_{i=1}^n(x_{i}\\beta - y_i)x_{i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our gradient step is \n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^{n+1} = \\beta^{n} - \\alpha\\nabla{L} =\\beta^{n} - \\alpha \\frac{1}{n}\\sum_{i=1}^n(x_{i}\\beta - y_i)x_{i}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning stops when for given $\\delta$\n",
    "\\begin{equation}\n",
    " \\left\\lVert  {\\beta^{n+1} - \\beta^{n}} \\right\\rVert  \\leq \\delta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "<br>\n",
    "* https://www.quora.com/What-are-some-real-world-applications-of-simple-linear-regression\n",
    "* https://analyticstraining.com/popular-applications-of-linear-regression-for-businesses/\n",
    "* http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf \n",
    "* https://en.wikipedia.org/wiki/Linear_regression#Assumptions\n",
    "* https://math.stackexchange.com/questions/1956541/mean-square-error-using-linear-regression-gives-a-convex-function\n",
    "* https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "* https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
